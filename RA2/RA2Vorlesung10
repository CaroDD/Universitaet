RA2 Vorlesung 10 Kapitel 9
Architekturkonzepte der Parallelverarbeitung

Gliederung
— Aspekte innovativer Architekturprinzipien
— Grundlegendes zu SIMD-Architekturen
— Vektorrechner
— Feldrechner
— Zellulare Systeme
— Datenflussarchitektur

Aspekte innovativer Architekturprinzipien

Architektonische Modelle der Parallelverarbeitung:
Aufbauend auf den unterschiedlichen Klassifizierungsverfahren sollen hier die Merkmale
zusammengestellt werden, die häufig für eine grobe Unterteilung der innovativen
Rechnerarchitekturen genutzt werden:

Steuerungsmodus:
— Innovative Prinzipien gehören nach Flynn zu den Klassen SIMD und MIMD
— Probleme beim MIMD-Modus:
	- Synchronisation der Abarbeitung und
	- Bereitstellung der Operanden vor Durchführung der Operation (Aktualisierungsmechanismen)

Speichergestaltung
— Man unterscheidet:
	- Zentralspeicher (Gemeinsamer Speicher, Globalspeicher)
	- Verteilter Speicher (Lokalspeicher)
— Strikt davon zu unterscheiden ist die Adressraum-Organisation:
	- Systemweit einheitlicher Adressraum = globaler Adressraum
	- Lokale Adressräume

Granularität
— Die Granularität entspricht der Anzahl der Teilaufgaben ohne Datenabhängigkeit ... mit
	entsprechender Anzahl von Verarbeitungseinheiten.
— Man unterscheidet grobe und feine Granularität
	- Feine Granularität mit einer hohen Anzahl von Verarbeitungseinheiten führt auf massiv-parallele Systeme
	- Dafür sind nur verteilte Speicher sinnvoll

Grundlegendes zu SIMD-Architekturen

Ausführung mit Pipelining (Wiederholung) (--> Folie 7)
Instruction Fetch	-->	Instruction Decode	--> 	Execute		-->	Writeback
• Lese Speicher an		• Interpretiere			• Benutze		• Schreibe Resultat in
  der Adresse des		  gelesenen Befehl		  Ausführungseinheit	  Register
  program counters/						  (z.B. ALU oder FPU)
  instruction pointers						  um Befehl auszuführen
		
Beispiel: Schleife, die ein Vektor auf einen Anderen addiert
for ( i = N ; i != 0 ; i-- )			
a[ i ] = a[ i ] + b [ i ];				
							  
my_loop:						
// load 1 element of a[]				 
movss (%r10,%ecx,$4), %xmm0;			— movss (%r10,%ecx,$4), %xmm0;			
							- Move Scalar Single: Transferiere einen single precision (32 bit) Wert
							- Von der Adresse, berechnet aus dem Inhalt von 2 Registern und Skalierungsfaktor: (%r10)+((%ecx)*4)
								Skalierungsfaktor ergibt sich aus Breite des Datums (32 Bit = 4 Byte)
							- Nach SSE Register xmm0
// add 1 element of b[]				— addss (%r11,%ecx,$4), %xmm0;	
addss (%r11,%ecx,$4), %xmm0;				- Add Scalar Single: Addiere einen single precision Wert
							- Von Adresse (%r11)+((%ecx)*4)
							- Auf einzelnen (gerade geladenen) Wert in xmm0

// store 1 element of a[]			— movss %xmm0, (%r10,%ecx,$4);	
movss %xmm0, (%r10,%ecx,$4);				- Speicher Ergebnis nach (%r10)+((%ecx)*4)

// decrement loop counter				
sub $1, %ecx;	
				
// jump if last result is not zero			
jnz my_loop;

Beispiel Schleife mit Pipelining (--> Folie 9)
Schleife: Benötigte Instruktionen
my_loop:
movss (%r10,%ecx,$4), %xmm0; 	// load 1 element of a[]
addss (%r11,%ecx,$4), %xmm0; 	// add 1 element of b[]
movss %xmm0, (%r10,%ecx,$4); 	// store 1 element of a[]
sub $1, %ecx; 			// decrement loop counter
jnz my_loop; 			// jump if last result is not zero
			
Für jede Schleifeniteration:
— 5 Instruktionen holen
— 5 Instruktionen dekodieren
— 5 Instruktionen ausführen
--> 5*N Instruktionen für N Schleifendurchläufe

SIMD (SSE)
moveups = Move Unaligned Packed Single: Transferiere (ohne besondere Anforderungen an die
	  genutzte Speicheradresse) 128 Bit als 4 single precision Werte

movups (%r10,%ecx,$4), %xmm0; // load 4 elements of a[] (unaligned)
addups (%r11,%ecx,$4), %xmm0; // add 4 elements of b[] (unaligned)
movups %xmm0, (%r10, %ecx, $4); // store 4 elements of a[] (unaligned)
sub $4, %ecx; // decrease loop counter ecx by 4
jnz my_loop; // jump if not zero
— Berechnen von 4 Schleifendurchläufen
	- Vorher: 4*5=20 Instruktionen
	- Nachher: 5 Instruktionen
— Nur Entlastung bei Instruction Fetch, Instruction Decode, es müssen immer noch die gleichen
Berechnungen und Datentransfers ausgeführt werden!

Maskenregister

Problem:
for ( i = N ; i >= 0 ; i-- )
if ( a[ i ] > 0 )
a[ i ] = a[ i ] + b [ i ];
--> Nicht die gleiche Operation auf allen Elementen anwendbar!
— Maskenregister maskieren Elemente, auf denen Operation nicht ausgeführt werden soll

Compilerunterstützung
Compiler stellen verschiedene Möglichkeiten für Programmierer zur Verfügung
— Assembler
— Intrinsics:
	- Nah an Assembler, Compiler übernimmt Registerallokation
— Präprozessor-Direktiven
	- Markieren von Schleifen für SIMD
— Auto-Vektorisierung
	- Compiler erkennt vektorisierbare Schleifen automatisch

Zusammenfassung
— Weniger Befehle --> weniger Instruction Fetch und Decode
— Mehr Fokus auf Execution
— Instruktion sagt, dass Berechnung der Elemente parallel stattfinden kann
— Implementierung ist hardwareabhängig

Vektorrechner und Feldrechner
Die parallele Berechnung der Elemente des Ergebnisvektors
	D = [a_0 * b_0 + c_0 	a_1 * b_1 + c_1 	...	a_(n-1) * b_(n-1) + c_(n-1) ]
kann auf zwei Arten erfolgen:
(1) Indem man die Terme a_i * b_i + c_i nach dem Pipeline-Prinzip berechnet, wozu man einen
	Multiplizierer und einen Addierer benötigt
	--> sog. Vektorrechner
oder
(2) Indem man n Rechenelemente hat, die in einem ersten Schritt alle Multiplikationen und in
	einem zweiten Schritt alle Additionen durchführen
	--> sog. Feldrechner

SIMD-Prinzipien (--> Folie 16)
— SIMD-Architekturen besitzen statische Verbindungsnetzwerke
— Der Datenaustausch in diesen Rechnern ist
	einfacher, weil auch das Verbindungsnetzwerk vom zentralen
	Steuerwerk gesteuert wird und mehr oder weniger synchron arbeitet

Vektorrechner
— Vektorrechner arbeiten nach dem Pipeline-Prinzip
	- Interne Verarbeitung wird durch sog. arithmetisches Pipelining realisiert
	- Vektorrechner wurden deshalb häufig als Pipeline-Rechner bezeichnet
— Da in aktuellen Rechnerarchitekturen das Pipelining auf unterschiedlichen Abstraktionsebenen
	eingesetzt wird, ist die Bezeichnung Pipeline-Rechner eher missdeutend!
— Das Pipeline-Prinzip gleicht einer Fließbandfertigung, in der Rechnerarchitektur taucht es unter
	verschiedenen Aspekten auf

Kenngrößen der Pipeline-Verarbeitung
- n: Anzahl der Prozesse, die durch die Pipeline laufen
- m: Anzahl der Segmente der Pipeline
- t_s : Segmentverarbeitungszeit
- k: (gleichlange) Teilprozesse
--> üblicherweise wird die Verarbeitung eines Prozesses durch die
	Segmente der Pipeline in m (gleichlange) Teilprozesse unterteilt (m = k)

Zum Geschwindigkeitsgewinn bei Pipelining (--> Folie 20)

Beschleunigungsfaktor (Speedup)
— Die Pipeline besitzt eine Einlaufphase, eine Phase mit höchster Parallelität und eine Auslaufphase.
— Eine effiziente Pipelineverarbeitung setzt voraus, dass eine möglichst hohe Anzahl von Prozessen
	benutzt werden. Bei nur einem Prozess entartet die Pipeline zu einer reinen sequentiellen Verarbeitung.
— Für einen Prozess gilt: T = k * t_s
— Für n Prozesse folgt: T_(seq) = n * k * t_s
— Für die Pipelineverarbeitung: T_(pip) = (k - 1) * t_s + n * t_s = (k - 1 + n) * t_s
	S_p = T_(seq) / T_(pip) = (n × k) / (k - 1 + n)
n = 1 => S_p = 1		— Pipeline-Rechner wirkt wie sequentieller Rechner

n >> k => S_p » k		— Je größer die Segmentierung (für Teilprozesse und Verarbeitung)
				  desto höher der Geschwindigkeitsgewinn

Berechnungsbeispiel
Berechnungsbeispiel zum Geschwindigkeitsgewinn durch Pipeline-Verarbeitung
— Wie viele Prozesse n müssen verarbeitet werden, damit bei einer Pipelineverarbeitung mit k = 3
  Teilprozessen in m = 3 Segmenten ein Geschwindigkeitsgewinn von 2 erreicht werden kann?

— k= 3	S_p = 2	n= ?

S_p = (n × k × t_s) / (k - 1 + n) t_s

n = (S_p * (k - 1)) / (k - S_p) = 4

— Sequentielle Verarbeitung:
	4 Prozesse --> 3 Zeitschritte = 12 Zeitschritte
— Pipeline-Verarbeitung
	4 Prozesse --> 1 Zeitschritt + (k-1) Schritte Einlaufzeit = 6 Zeitschritte

Basis zu den Vektorrechnern
— Pionierarbeit bei den Vektorrechnern hat die Firma Control Data Corporation (CDC) geleistet
	- Erste Vektormaschine war die STAR-100 Anfang der 70er Jahre von CDC
	- Ende der 70er Jahre wurde die STAR-100 von der CYBER 203 von CDC abgelöst
— Mit der CRAY-1 (1976) begann die Ära der Firma CRAY RESEARCH (1972 gegründet)
	- Spezialität: Spitzenleistung auf dem Gebiet der Halbleitertechnologie (Taktfrequenzen, Speicherzugriffszeiten)

Fujitsu VP-200 als Beispiel eines Vektorrechners
— Gründe, weshalb dieses System hier vorgestellt wird:
	- Vektorrechnerprinzip ist hier noch gut als separates Prinzip erkennbar
	- SNI VP200EX arbeitete von 1991 bis August 1995 als sog. Landesvektorrechner im Universitätsrechenzentrum der TU Dresden
	- Von Fujitsu gab es bereits 1992 einen Vektorprozessor als Ein-Chip-Mikroprozessor
— Ende 1983 gab es erste Modelle des Fujitsu Vektorprozessors FACOM VP-100 und VP-200
— Fujitsu-Rechner wurden in Europa über FSC vermarktet und betreut
— Hinweis:
	- Ein Vektorbefehlssatz ist beispielsweise in [Hen07] für die DLXV-Architektur dargestellt

Aufbau des VP-200 Vektorprozessors
--> Folie 27-31

— VP-200: 1 Vektor- und 1 Skalarprozessor, also MIMD/SIMD-Hybrid-Architektur
— Die Hauptspeicherkapazität betrug ursprünglich 256 MByte, zum Produktionsende max. 1 GByte
— Vektorprozessor besteht aus einer skalaren und einer vektoriellen Einheit
	- Skalareinheit holt und dekodiert alle Instruktionen:
	- Skalare Befehle --> Skalareinheit	|
						|	Parallelarbeit möglich!
	- Vektorielle Befehle --> Vektoreinheit	|

— In der Skalareinheit befinden sich:
	- 16 allgemeine Register (= General Purpose Register)
	- 8 Gleitkomma-Register
	- 64 KByte Cache
— Die Vektoreinheit des VP-200 enthält:
	- 64 KByte dynamisch rekonfigurierbare Vektorregister, die in Anzahl und Länge dem auszuführenden Programm angepasst werden
	- Kombinationen:
		- 256 Register für je 32 Elemente zu je 8 Byte (=64 Bit)
		- 8 Register für je 1024 Elemente zu je 8 Byte (=64 Bit)
		- 256 32bit-Maskenregister, d.h. jedes der 256*32=8192 = 8K Elemente ist einzeln maskierbar = 1 KByte
	Maskenregister-Gesamtgröße
	--> Besondere Bedeutung, wenn IF-Anweisungen in Schleifen einhalten sind: Ausblenden einzelner
		Iterationen über Maskierung

Von den 3 segmentierten arithmetischen Funktionseinheiten (Segmentierung = Pipelines)
- Addition	 267 MFLOPS
- Multiplikation 267 MFLOPS
- Division	 38 MFLOPS
können zwei (VP-100; VP-200: pro CPU) bzw. drei (VP-200e) parallel arbeiten.

Zusätzliche Parallelität wird in der Vektoreinheit ermöglicht durch
- die MASK-Einheit und
- die beiden LOAD-/STORE-Kanäle bidirektional

— Speicherbandbreite
Datenbandbreite = 32 Byte (Speicherbus-Breite) / 15ns (Speicherbus-Taktzeit) = 2,13 Gbyte/s
— VP-200 hatte als erster Supercomputer aufgrund der MASK-Unit und der Maskenregister die
Möglichkeit, arithmetische Operationen unter Steuerung einer logischen Maske auszuführen
	
Besondere Technik bei Vektorrechnern: Chaining
--> Folie 32

Speedup durch Chaining
S_p = T_P (ohne Chaining) / T_P (mit Chaining)
S_p = (k1 - 1 + n + k2 - 1 + n) / ((k1+k2) - 1 + n)
n → ∞ => S_p = 2

Banking (Speicherverschränkung; Interleaved Memory)
— Problem:
	- Zu große Speicherzugriffszeiten beim Einsatz preisgünstiger Speicher
— Lösung:
	- Anordnung des Hauptspeichers in Speicherbänken
	- Aufeinanderfolgende Speicherinhalte werden separaten physischen Speicherbänken zugeordnet
--> Folie 34

Arithmetisches Pipelining
--> Folie 35 - 38

SX-4 Block Diagram (Vector)
--> Folie 39 - 41

Processor Technology
--> Folie 42

Levels of Parallelism
— Segmentation
— Multiple pipelines (8/16-fold)
— Parallel usage of functional units
— Parallel CPUs
— Parallel nodes

Measurements
--> Folie 46

Feldrechner

Feldrechner (--> Folie 48)
— Feldrechner bestehen aus einem Feld regelmäßig verbundener Verarbeitungseinheiten, die
	eine Maschinenoperation auf verschiedene Daten anwenden
— Ersatz des Kooperationsprinzips der Selbstbestimmung der Zellen durch das Master-Slave-Prinzip

Grundschema eines Feldrechners
--> Folie 49

Entwicklungslinie:
Ausgangspunkt für diese Rechnerarchitektur war der SOLOMON-Rechner der University of Illinois,
der Anfang der 60er Jahre aufgebaut wurde:
— Matrix von 32 x 32 = 1024 1-Bit-Rechenelementen unter Steuerung einer zentralen Steuereinheit (Control Unit - CU)
— Jedes Rechenelement (RE) besitzt einen kleinen privaten Speicher von 4096 Bit = 4 KBit
— Auszuführendes SOLOMON-Programm befindet sich in der zentralen Steuereinheit
— Zu jedem gegebenen Zeitpunkt können alle Rechenelemente dieselbe Operation auf einem
   Operanden ausführen, der in allen Rechenelement-Speichern dieselbe Adresse hat
— Einzige Autonomie der Rechenelemente besteht darin, sich von einer Operationsausführung
   suspendieren zu können
— Jedes RE kann mit seinen vier unmittelbaren Nachbarn verkehren; Randelemente für E/A-Aufgaben

Thinking Machines Corporation
Thinking Machines Corporation (TMC) Connection Machine CM-2 bzw. CM-200 als Beispiel eines
Feldrechners
— Gründe, weshalb dieses System hier vorgestellt wird:
	- CM-2 (bzw. CM-200) ist einer der insgesamt erfolgreichsten Supercomputer vom Array-Typ
	- Produktpolitik und Firmengeschichte von TMC spiegelt die Probleme des Supercomputing eindrucksvoll wider
	- Im Maximalausbau umfasst die CM-2 4 Quadranten mit je 16 K Rechenelementen = 64 K RE (Minimalausbau: 2 K RE)
	- Jeder Quadrant wird von einem eigenen Mikroprogrammsteuerwerk mit Befehlen versorgt
	- Das System kann bis zu 4 Vorrechner (üblicherweise SUN-Workstations) besitzen (Spacesharing) und ist
	  somit auch eine MIMD-SIMD-Hybrid-Architektur
	- Aufgaben der Vorrechner: Benutzerschnittstelle, Programmübersetzung, I/O-Aufgaben (Sekundärspeicher, Netzwerk)

TMC CM-2
--> Folie 52

Beschreibung der CM-2
— RE sind 1-Bit-ALUs, die bitseriell arbeiten
— Jedes RE besitzt 4 KByte lokalen Speicher und ein Statusregister, über welches ein RE von der
   Bearbeitung eines Befehls ausgeschlossen werden kann
— Die Kommunikation mit den anderen RE erfolgt ebenfalls über das Statusregister
— 16 RE sind zusammen mit der Hardware für die Datenweg-Vermittlung (Router) und 4
   Speicherbänken mit je 16 KByte auf einem Chip untergebracht = 1 Cluster
— Ein Quadrant besteht aus 1024 solcher Cluster mit je 16 RE (=16 K RE/Quadrant = 2 Würfel- Gehäuse)
— RE des Gesamtsystems können wahlweise auf zwei Wegen miteinander verbunden werden:
	- NEWS-Gitter:
		- 2-dimensionales, rechteckiges Gitter
		- Jedes RE ist mit seinen vier unmittelbaren Nachbarn verbunden
			* günstig bei hoher Datenlokalität: sehr schnell zum Nachbarn
			* auch zu entfernten RE möglich über store-and-forward-Verfahren
	- Hypercube-Verbindungsnetzwerk:
		- Topologie eines 16-dimensionalen Würfels
		- Wormhole-Routing-Verfahren (Übertragungszeit in erster Näherung unabhängig von der Distanz der Kommunikationspartner)
— Entscheidung trifft der Programmierer!

Blockbild eines Clusters und eines Rechenelementes
--> Folie 55

Beschreibung der CM-2
— CM-2 war ursprünglich für nicht-numerische Anwendungen konzipiert worden, insbesondere für KI-Probleme
— Deshalb ursprünglich hauptsächlich Programmiersprache LISP
— Probleme:
	- Parallelisierung der LISP-Programme
	- Fehlender Markt für nicht-numerische Supercomputer
— Weiterentwicklung der CM-2, indem je 32 RE (^=32 Bit) ein 64 Bit-Gleitkommaprozessor der Fa. Weitek zugeordnet wurde
	--> max. 2000 FP-Prozessoren!
— Die Rolle der RE reduziert sich damit hauptsächlich darauf, die Parallelarbeit der Gleitkomma-
   Prozessoren zu steuern und die Datenkommunikation zwischen ihnen durchzuführen
— Zusätzlich 4 MBit Dual-Port-Speicher zu je 32 RE und einem FP-Prozessor
— Daten vom Hostspeicher können über Broadcasting gleichzeitig an alle Rechenwerke gesendet werden
— Ergebnisse von Berechnungen auf Rechenwerken können durch logische Operationen, wie AND
   oder OR, zu einem Wert verknüpft werden, der an den Host zurückgeschickt werden kann
— Der direkte Datenaustausch zwischen dem Host und jeweils einem RE ist möglich
— Über den globalen Bus kann auch der Mikrocontroller direkt auf die Daten in den Speicherchips
   in den Knoten zugreifen
— Hauptsächliche Programmiersprachen:
	- FORTRAN
	- C* (Erweiterung der Sprache C um Vektorkonstrukte)
— Verbesserung der CM-2 zur CM-200 durch Senkung der Taktzykluszeit von 142 ns auf 100 ns (10 MHz)
	- Peak Performance: 20 GFLOPS bei 2K FP-Prozessoren, d.h. 10 MFLOPS pro FP-Prozessor
	- Linpack max: 9,8 GFLOPS
	- Markteinführung: 1991 (kurz nach CM-5-Ankündigung)

Knotenstruktur der CM-200
--> Folie 59

ILLIAC IV als Beispiel eines Feldrechners
--> Folie 60

Aus dem SIMD-Prinzip abgeleitete Rechnerarchitekturen
SIMD-Erweiterungen (MMX, SSE, AVX, AVX 512) bei Standard-Prozessoren
— Anleihe beim Feldrechner
	- Gleiche Operation wird gleichzeitig auf mehreren Operanden durchgeführt 

GPGPUs
— Innerhalb eines Warp strenges SIMD-Feldrechnerprinzip

Zellulare Systeme
— Wie Feldrechner aus einem Feld von Verarbeitungseinheiten aufgebaut
— Verarbeitungseinheiten können jedoch zum selben Zeitpunkt verschiedene Operationen ausführen
— Unterscheiden sich vom Feldrechner auch durch dezentrale Steuerung

Aus dem SIMD-Prinzip abgeleitete Rechnerarchitekturen:
Systolische Arrays
— Besitzen ein Feld von Verarbeitungseinheiten mit regelmäßiger Verbindungsstruktur
— Meist synchroner Takt vorgesehen
	- Daten werden zu bestimmten Taktzeitpunkten an lokale Nachbarn weitergereicht
	--> Datenströme pulsieren auf diese Weise durch das systolische Array

Zellulare Systeme

Charakteristik Zellularer Systeme
Jede Zelle ist über Daten- und Aktivierungsleitungen mit Nachbarzellen verbunden
— Es können mehrere Programme gleichzeitig ausgeführt werden
— Zellen werden nicht wie Speicherzellen adressiert (da keine zentrale Kontrolle), sondern ein Programmschritt besteht immer aus:
	- Wegbildungsphase
	- Ausführungsphase

Programm besteht aus einer Menge von Daten und einer Menge von Steuerwörtern.

Steuerwort enthält:
	- Operationscode und
	- Anweisung zur Wegbildung

Zellulare Systeme
--> Folie 65

Eigenschaften zellularer Systeme
— Einsatzspektrum ist auf wohlstrukturierte Spezialanwendungen (Bildverarbeitung,
   Mustererkennung) begrenzt
— Broadcast ist nicht ausgeschlossen
— Rippling ist nicht ausgeschlossen
	- Rippling (Plätschern): Ausbreitung eines Eingangswertes wie eine Welle durch die Anordnung aufgrund
	  der Verzögerungszeit der jeweiligen Zelle; keine einheitliche Verzögerungszeit, sondern von Funktion abhängig

Systolische Arrays
— Das Gebiet der systolischen Arrays ist eine interessante Entwicklungsrichtung der parallelen Datenverarbeitung
— Der pulsierende Datentransfer führt auf einen Vergleich zum Blutkreislauf
— Die für die Bearbeitung auf systolischen Arrays geeigneten Algorithmen werden als systolische
   Algorithmen bezeichnet

Charakteristik Systolischer Arrays
— Sie besitzen ein Feld von Verarbeitungseinheiten mit regelmäßiger Verbindungsstruktur (meist
  eine Gitterstruktur mit Verbindungen eines jeden Verarbeitungselementes zu vier oder sech Nachbarknoten)
— In der einfachsten Variante der systolischen Arrays läuft in jedem Verarbeitungselement immer
   dasselbe Programm ab
— Verschiedene Verarbeitungselemente können jedoch parallel unterschiedliche Befehle ausführen
— Meist ist ein synchroner Takt vorgesehen, wobei die Daten zu bestimmten Taktzeitpunkten an
   lokale Nachbarn weitergereicht werden, um auf diese Weise Datenströme durch das systolische
   Array pulsieren zu lassen (daher der Name)
— Es gibt keine zentrale Systemaufsicht und keine Broadcast-Möglichkeit für Daten oder Befehle

Modell nach Kung und Leiserson
--> Folie 69

Definition eines Systolischen Arrays
Definition eines Systolischen Arrays nach Petkov:
— Ein systolisches Array ist ein zellularer Automat, der aus gleichartigen und im Raum gleichmäßig
  angeordneten Zellen besteht, die über ein regelmäßiges und lokales Kopplungsmuster
  verbunden sind, wobei kein Rippling und kein Broadcasting auftreten.

Möglichkeiten der Anordnung in systolischen Arrays
--> Folie 71

Allgemeine Darstellung der Zellfunktionen
--> Folie 72

Zellfunktionen für Formatumwandlungen
--> Folie 73 & 74

Datenflussarchitektur

Datenflussarchitektur (Folie 78)
— Ein Datenflussrechner arbeitet nach dem Datenflussprinzip
— Steuerung des Ablaufs nicht explizit durch Kontrollfluss bestimmt, sondern implizit aus dem
  Datenfluss abgeleitet

Besonderheiten
— Hervorhebung:
	- Single Assignment-Prinzip der Speicherzellen
	- Logische Variable: leer/besetzt
	- Speicherplatz nur 1 mal besetzbar
— Die Hauptprobleme der Multiprozessorsysteme auf Architekturebene betreffen die
   Speicherlatenz und die Synchronisation paralleler Kontrollfäden
— Das Datenflussprinzip erlaubt, beliebige Verzögerungen beim Zugriff auf entfernte Speicher zu
   tolerieren und die Daten in beliebiger Reihenfolge vom Speicher anzuliefern

Beispiel für das Datenfluss-Prinzip
--> Folie 80

Grundarchitektur einer dynamischen Datenflussmaschine
--> Folie 81






